{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [[  nan   nan   nan ...,   nan   nan   nan]\n [-0.3   1.71  0.3  ...,  0.55  0.55  1.38]\n [ 1.38 -0.3   1.71 ...,  0.73  0.55 -0.26]\n ..., \n [ 1.64 -0.21 -1.89 ...,  0.07  0.99  1.8 ]\n [ 1.8   1.64 -0.21 ...,  2.28  0.07 -0.16]\n [-0.16  1.8   1.64 ...,  1.22  2.28 -0.7 ]]\nX: [[-0.3   1.71  0.3  ...,  0.73  0.55  0.55]\n [ 1.38 -0.3   1.71 ..., -1.55  0.73  0.55]\n [-0.26  1.38 -0.3  ..., -0.06 -1.55  0.73]\n ..., \n [ 1.64 -0.21 -1.89 ...,  2.28  0.07  0.99]\n [ 1.8   1.64 -0.21 ...,  1.22  2.28  0.07]\n [-0.16  1.8   1.64 ..., -1.87  1.22  2.28]]\nY: [  1.38000000e+00  -2.60000000e-01   1.44000000e+00  -1.26000000e+00\n  -1.34000000e+00   4.40000000e-01   1.76000000e+00  -6.40000000e-01\n   5.50000000e-01   1.88000000e+00   1.23000000e+00   4.50000000e-01\n  -3.10000000e-01  -5.60000000e-01  -2.04000000e+00  -1.20000000e-01\n  -1.00000000e-01  -5.40000000e-01   2.54000000e+00   6.30000000e-01\n   8.40000000e-01   1.32000000e+00   1.28000000e+00   1.42000000e+00\n   2.08000000e+00  -3.13000000e+00  -9.10000000e-01   9.00000000e-02\n   8.80000000e-01   3.57000000e+00   1.93000000e+00   2.04000000e+00\n   2.10000000e+00   4.00000000e-01  -1.33000000e+00  -3.16000000e+00\n  -1.80000000e+00  -1.11000000e+00   2.11000000e+00  -2.98000000e+00\n  -4.11000000e+00   1.58000000e+00   1.46000000e+00  -2.41000000e+00\n   1.53000000e+00   2.68000000e+00   1.08000000e+00   8.10000000e-01\n  -1.66000000e+00  -9.10000000e-01   1.70000000e+00   3.52000000e+00\n   1.47000000e+00   1.47000000e+00   1.68000000e+00  -5.90000000e-01\n  -3.40000000e-01   2.80000000e-01  -3.79000000e+00   1.46000000e+00\n  -5.70000000e-01  -1.90000000e+00  -1.20000000e-01  -2.64000000e+00\n   2.82000000e+00   3.28000000e+00  -9.00000000e-02   2.53000000e+00\n   2.87000000e+00  -8.70000000e-01   2.06000000e+00  -1.22000000e+00\n  -1.63000000e+00   2.10000000e+00   2.04000000e+00   3.31000000e+00\n  -7.00000000e-02   1.58000000e+00   2.18000000e+00  -3.87000000e+00\n  -8.70000000e-01   2.36000000e+00  -1.70000000e-01  -2.98000000e+00\n  -4.00000000e-02  -7.00000000e-01  -1.18000000e+00  -8.10000000e-01\n  -1.65000000e+00  -1.68000000e+00   1.41000000e+00  -6.30000000e-01\n   2.03000000e+00   4.90000000e-01  -2.19000000e+00   1.72000000e+00\n   3.55000000e+00   3.40000000e-01   3.08000000e+00   2.00000000e-01\n  -4.30000000e-01  -3.54000000e+00  -3.68000000e+00   2.03000000e+00\n   8.90000000e-01  -1.39000000e+00   1.79000000e+00  -4.90000000e-01\n   3.90000000e+00   9.80000000e-01   1.85000000e+00   3.29000000e+00\n  -1.00000000e-02  -6.00000000e-02  -5.50000000e-01  -2.47000000e+00\n   1.59000000e+00   7.90000000e-01   8.00000000e-02  -2.09000000e+00\n   5.00000000e-01   2.68000000e+00   2.14000000e+00   4.80000000e-01\n  -1.49000000e+00   3.00000000e-01   9.70000000e-01   5.30000000e-01\n  -3.67000000e+00   4.10000000e-01  -2.38000000e+00  -2.54000000e+00\n  -3.44000000e+00  -2.37000000e+00   6.10000000e-01  -2.44000000e+00\n  -1.28000000e+00  -1.30000000e-01   1.76000000e+00  -1.57000000e+00\n   1.35000000e+00  -2.37000000e+00   1.86000000e+00   2.67000000e+00\n   2.50000000e-01  -4.10000000e-01   9.20000000e-01  -2.07000000e+00\n  -4.02000000e+00  -2.62000000e+00  -2.64000000e+00   2.76000000e+00\n   2.85000000e+00   1.90000000e+00  -5.40000000e-01   1.61000000e+00\n   1.60000000e+00  -8.10000000e-01  -2.92000000e+00   1.45000000e+00\n   4.98000000e+00   4.50000000e-01  -2.23000000e+00   4.30000000e-01\n   2.56000000e+00  -1.79000000e+00  -1.89000000e+00   1.71000000e+00\n   1.68000000e+00  -1.35000000e+00   1.96000000e+00   7.60000000e-01\n   2.18000000e+00   4.00000000e-01   2.55000000e+00   1.02000000e+00\n   2.14000000e+00   3.20000000e-01   1.52000000e+00  -4.84000000e+00\n   1.20000000e-01  -1.91000000e+00   2.77000000e+00   3.03000000e+00\n   3.01000000e+00   2.83000000e+00  -7.30000000e-01  -4.80000000e-01\n   8.90000000e-01   6.37000000e+00   6.10000000e-01   5.12000000e+00\n   2.20000000e+00  -2.09000000e+00   3.36000000e+00  -4.55000000e+00\n  -3.76000000e+00   2.43000000e+00   1.90000000e-01   4.33000000e+00\n   2.49000000e+00  -3.47000000e+00  -3.17000000e+00  -2.29000000e+00\n   1.89000000e+00  -2.04000000e+00   5.03000000e+00   5.33000000e+00\n   1.39000000e+00   2.58000000e+00   6.00000000e+00  -4.06000000e+00\n  -1.12000000e+00  -7.50000000e-01   6.25000000e+00   4.67000000e+00\n   3.54000000e+00  -2.61000000e+00   7.66000000e+00   1.93000000e+00\n   5.53000000e+00  -2.00000000e+00  -3.20000000e-01   6.59000000e+00\n  -3.60000000e-01   3.26000000e+00   4.83000000e+00  -2.42000000e+00\n  -4.70000000e+00  -8.90000000e+00  -1.87000000e+00  -5.69000000e+00\n  -4.43000000e+00   1.18000000e+00   4.90000000e-01  -7.82000000e+00\n  -5.45000000e+00  -5.56000000e+00   1.23400000e+01  -1.29500000e+01\n  -1.04600000e+01  -1.10100000e+01  -6.43000000e+00  -3.34000000e+00\n  -1.13000000e+00  -5.67000000e+00  -6.14000000e+00   1.04000000e+00\n  -8.04000000e+00  -7.90000000e-01  -5.06000000e+00  -1.66000000e+00\n   4.51000000e+00   2.01000000e+00  -7.60000000e+00   6.20000000e+00\n  -5.90000000e-01  -2.02000000e+00  -3.59000000e+00   8.00000000e-02\n   5.19000000e+00   1.08000000e+00   2.42000000e+00   3.78000000e+00\n   3.99000000e+00  -3.13000000e+00   1.40000000e-01  -6.60000000e-01\n  -1.32000000e+00   2.59000000e+00   4.93000000e+00   2.07000000e+00\n   2.62000000e+00   3.63000000e+00   3.78000000e+00   2.73000000e+00\n  -2.20000000e-01  -1.76000000e+00   5.10000000e-01  -7.88000000e+00\n  -2.10000000e-01   4.59000000e+00   1.21000000e+00   4.50000000e+00\n  -1.92000000e+00   1.31000000e+00   1.66000000e+00  -4.21000000e+00\n   2.50000000e+00   4.50000000e-01  -2.93000000e+00   5.60000000e-01\n   1.87000000e+00   4.88000000e+00   4.61000000e+00  -1.84000000e+00\n   5.30000000e-01  -8.30000000e-01   1.50000000e-01  -1.46000000e+00\n  -6.00000000e-02  -5.31000000e+00   2.40000000e-01   3.63000000e+00\n   3.67000000e+00   3.30000000e+00  -2.30000000e+00  -3.25000000e+00\n  -2.79000000e+00   5.60000000e-01  -6.80000000e-01   4.39000000e+00\n   1.07000000e+00   8.20000000e-01   1.57000000e+00  -3.00000000e-01\n  -6.00000000e-01   2.43000000e+00   2.64000000e+00  -1.33000000e+00\n  -1.12000000e+00   7.60000000e-01  -4.02000000e+00  -5.25000000e+00\n  -5.49000000e+00   2.32000000e+00   1.09000000e+00   5.20000000e-01\n   3.33000000e+00   6.10000000e-01  -2.37000000e+00  -5.80000000e-01\n   1.95000000e+00   1.41000000e+00   4.00000000e-01   3.65000000e+00\n  -3.63000000e+00  -3.32000000e+00  -1.70000000e+00   8.70000000e-01\n   8.40000000e-01   7.80000000e-01  -7.00000000e-01   3.47000000e+00\n   3.95000000e+00  -3.10000000e-01  -7.40000000e-01   8.00000000e-01\n   2.86000000e+00   1.88000000e+00  -4.66000000e+00   4.50000000e-01\n   4.11000000e+00   1.74000000e+00  -2.50000000e-01   1.91000000e+00\n   8.10000000e-01  -1.44000000e+00   1.50000000e+00  -9.80000000e-01\n  -2.78000000e+00   3.41000000e+00  -3.99000000e+00  -1.35000000e+00\n   1.13700000e+01   4.32000000e+00   2.71000000e+00  -3.97000000e+00\n   4.85000000e+00   9.20000000e-01   4.21000000e+00  -1.54000000e+00\n   1.54000000e+00   3.06000000e+00  -6.65000000e+00  -5.51000000e+00\n  -2.19000000e+00   1.43000000e+00   1.01000000e+00  -8.90000000e-01\n  -4.12000000e+00  -3.04000000e+00   8.80000000e-01   3.37000000e+00\n  -3.90000000e-01   1.40000000e+00  -2.40000000e-01  -7.05000000e+00\n  -7.90000000e+00   2.43000000e+00  -2.70000000e-01   2.99000000e+00\n  -1.60000000e-01   1.02000000e+00  -5.16000000e+00  -2.32000000e+00\n  -1.74000000e+00   5.85000000e+00   1.15000000e+00   5.67000000e+00\n   8.60000000e-01   3.73000000e+00   2.30000000e+00  -2.30000000e+00\n   2.93000000e+00   2.00000000e-01  -4.05000000e+00   1.75000000e+00\n   2.00000000e+00   2.59000000e+00  -1.95000000e+00  -4.00000000e-01\n  -6.00000000e-01  -1.63000000e+00   7.00000000e-01   3.29000000e+00\n   5.63000000e+00   1.10000000e-01  -1.23000000e+00  -1.80000000e-01\n   5.10000000e-01  -1.53000000e+00  -1.60000000e+00  -9.80000000e-01\n   4.90000000e-01   1.03000000e+00  -5.90000000e-01  -6.47000000e+00\n  -3.87000000e+00  -1.99000000e+00  -4.04000000e+00  -2.65000000e+00\n  -1.12000000e+00  -1.91000000e+00  -1.12000000e+00   5.48000000e+00\n   1.00000000e-02   4.54000000e+00  -1.29000000e+00   3.00000000e-02\n   4.03000000e+00   1.33000000e+00   2.05000000e+00  -7.90000000e-01\n  -3.00000000e-02   1.96000000e+00  -3.88000000e+00  -2.27000000e+00\n  -6.40000000e-01   5.60000000e-01   2.60000000e-01  -4.95000000e+00\n  -8.10000000e-01   1.10000000e-01  -1.10000000e-01   2.04000000e+00\n  -2.40000000e-01  -1.50000000e-01  -1.38000000e+00   2.54000000e+00\n   1.63000000e+00   2.42000000e+00   6.20000000e-01   1.18000000e+00\n   1.29000000e+00   1.61000000e+00  -1.24000000e+00   7.40000000e-01\n  -2.67000000e+00  -2.02000000e+00  -1.27000000e+00   1.74000000e+00\n   2.80000000e-01   3.24000000e+00  -1.31000000e+00  -1.53000000e+00\n  -5.43000000e+00   3.23000000e+00   2.52000000e+00   2.45000000e+00\n  -1.20000000e+00   1.40000000e-01  -1.68000000e+00   8.30000000e-01\n   2.05000000e+00   4.00000000e-01  -7.90000000e-01   4.59000000e+00\n   4.30000000e+00   2.17000000e+00  -1.04000000e+00  -4.40000000e-01\n  -3.80000000e-01   1.80000000e+00  -1.45000000e+00   2.83000000e+00\n   3.70000000e-01  -4.10000000e-01  -2.03000000e+00  -3.17000000e+00\n   6.00000000e-02  -4.90000000e-01  -1.15000000e+00  -3.71000000e+00\n  -8.40000000e-01  -2.61000000e+00  -3.90000000e-01   7.00000000e-02\n  -8.00000000e-01   3.22000000e+00   1.06000000e+00   6.40000000e-01\n   1.38000000e+00  -2.72000000e+00  -4.02000000e+00   6.20000000e-01\n   3.04000000e+00   8.20000000e-01   5.00000000e-01   2.46000000e+00\n   2.52000000e+00  -2.70000000e-01   3.20000000e-01  -3.52000000e+00\n   1.60000000e-01   9.90000000e-01   7.00000000e-02   2.28000000e+00\n   1.22000000e+00  -1.87000000e+00  -1.89000000e+00  -2.10000000e-01\n   1.64000000e+00   1.80000000e+00  -1.60000000e-01  -7.00000000e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients: [ 0.1890249   0.01974564 -0.00976762  0.03666562  0.02590676  0.02619397\n -0.00377285  0.16312961]\nintercept: 0.0687652667874\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[102   6].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-edbbf69b025f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mx_pre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m102\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0my_pre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"Y-Predict:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pre\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \"\"\"\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coef_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m    241\u001b[0m                                dense_output=True) + self.intercept_\n",
      "\u001b[1;32mD:\\Applications\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[102   6].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn import linear_model  \n",
    "import numpy as np  \n",
    "from numpy import genfromtxt #可以将非array格式的list转化为array  \n",
    "  \n",
    "datapath = \"week-wti.csv\"  \n",
    "deliverData = genfromtxt(datapath,delimiter=\",\") #将csv文件转化为numpy.array格式  \n",
    "  \n",
    "print(\"data:\",deliverData)  \n",
    "  \n",
    "X= deliverData[1:, 0:8].astype(np.float)\n",
    "Y = deliverData[1:, 8].astype(np.float)\n",
    "print( \"X:\",X  ) \n",
    "print( \"Y:\",Y  ) \n",
    "  \n",
    "regr = linear_model.LinearRegression()  \n",
    "regr.fit(X,Y)  \n",
    "  \n",
    "print( \"coefficients:\",regr.coef_ )       #与X结合的值  \n",
    "print( \"intercept:\",regr.intercept_  )       #类似于截距  \n",
    "  \n",
    "x_pre = [102,6]  \n",
    "y_pre = regr.predict(x_pre)  \n",
    "print( \"Y-Predict:\",y_pre  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anaconda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "\n",
    "raw_data = urllib.urlopen(url)\n",
    "# load the CSV file as a numpy matrix\n",
    "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
    "# separate the data from the target attributes\n",
    "X = dataset[:,0:7]\n",
    "y = dataset[:,8]\n",
    "winequality-white\n",
    "tmp = np.loadtxt(\"train.csv\", dtype=np.str, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tmp = np.loadtxt(\"winequality-white.csv\", dtype=np.str, delimiter=\";\")\n",
    "data = tmp[1:,0:11].astype(np.float)#加载数据部分\n",
    "label = tmp[1:,11].astype(np.float)#加载类别标签部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  7.  ,   0.27,   0.36, ...,   3.  ,   0.45,   8.8 ],\n        [  6.3 ,   0.3 ,   0.34, ...,   3.3 ,   0.49,   9.5 ],\n        [  8.1 ,   0.28,   0.4 , ...,   3.26,   0.44,  10.1 ],\n        ..., \n        [  6.5 ,   0.24,   0.19, ...,   2.99,   0.46,   9.4 ],\n        [  5.5 ,   0.29,   0.3 , ...,   3.34,   0.38,  12.8 ],\n        [  6.  ,   0.21,   0.38, ...,   3.26,   0.32,  11.8 ]]),\n array([ 6.,  6.,  6., ...,  6.,  7.,  6.]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0x90 in position 614: ordinal not in range(128)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3f74c1e0f3ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;34m'reading training and testing data...'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m     \u001b[0mnum_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[0mnum_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-3f74c1e0f3ec>\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(data_file)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0x90 in position 614: ordinal not in range(128)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pickle as pickle\n",
    "\n",
    "#importlib.reload(sys)\n",
    "#sys.setdefaultencoding('utf8')\n",
    "\n",
    "\n",
    "# Multinomial Naive Bayes Classifier\n",
    "def naive_bayes_classifier(train_x, train_y):\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    model = MultinomialNB(alpha=0.01)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# KNN Classifier\n",
    "def knn_classifier(train_x, train_y):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "def logistic_regression_classifier(train_x, train_y):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression(penalty='l2')\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Random Forest Classifier\n",
    "def random_forest_classifier(train_x, train_y):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(n_estimators=8)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Decision Tree Classifier\n",
    "def decision_tree_classifier(train_x, train_y):\n",
    "    from sklearn import tree\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier\n",
    "def gradient_boosting_classifier(train_x, train_y):\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    model = GradientBoostingClassifier(n_estimators=200)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# SVM Classifier\n",
    "def svm_classifier(train_x, train_y):\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC(kernel='rbf', probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# SVM Classifier using cross validation\n",
    "def svm_cross_validation(train_x, train_y):\n",
    "    from sklearn.grid_search import GridSearchCV\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC(kernel='rbf', probability=True)\n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs=1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for para, val in best_parameters.items():\n",
    "        print\n",
    "        para, val\n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def read_data(data_file):\n",
    "    import gzip\n",
    "    f = gzip.open(data_file, \"rb\")\n",
    "    train, val, test = pickle.load(f)\n",
    "    f.close()\n",
    "    train_x = train[0]\n",
    "    train_y = train[1]\n",
    "    test_x = test[0]\n",
    "    test_y = test[1]\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_file = \"mnist.pkl.gz\"\n",
    "    thresh = 0.5\n",
    "    model_save_file = None\n",
    "    model_save = {}\n",
    "\n",
    "    test_classifiers = ['NB', 'KNN', 'LR', 'RF', 'DT', 'SVM', 'GBDT']\n",
    "    classifiers = {'NB': naive_bayes_classifier,\n",
    "                   'KNN': knn_classifier,\n",
    "                   'LR': logistic_regression_classifier,\n",
    "                   'RF': random_forest_classifier,\n",
    "                   'DT': decision_tree_classifier,\n",
    "                   'SVM': svm_classifier,\n",
    "                   'SVMCV': svm_cross_validation,\n",
    "                   'GBDT': gradient_boosting_classifier\n",
    "                   }\n",
    "\n",
    "    print\n",
    "    'reading training and testing data...'\n",
    "    train_x, train_y, test_x, test_y = read_data(data_file)\n",
    "    num_train, num_feat = train_x.shape\n",
    "    num_test, num_feat = test_x.shape\n",
    "    is_binary_class = (len(np.unique(train_y)) == 2)\n",
    "    print\n",
    "    '******************** Data Info *********************'\n",
    "    print\n",
    "    '#training data: %d, #testing_data: %d, dimension: %d' % (num_train, num_test, num_feat)\n",
    "\n",
    "    for classifier in test_classifiers:\n",
    "        print\n",
    "        '******************* %s ********************' % classifier\n",
    "        start_time = time.time()\n",
    "        model = classifiers[classifier](train_x, train_y)\n",
    "        print\n",
    "        'training took %fs!' % (time.time() - start_time)\n",
    "        predict = model.predict(test_x)\n",
    "        if model_save_file != None:\n",
    "            model_save[classifier] = model\n",
    "        if is_binary_class:\n",
    "            precision = metrics.precision_score(test_y, predict)\n",
    "            recall = metrics.recall_score(test_y, predict)\n",
    "            print\n",
    "            'precision: %.2f%%, recall: %.2f%%' % (100 * precision, 100 * recall)\n",
    "        accuracy = metrics.accuracy_score(test_y, predict)\n",
    "        print\n",
    "        'accuracy: %.2f%%' % (100 * accuracy)\n",
    "\n",
    "    if model_save_file != None:\n",
    "        pickle.dump(model_save, open(model_save_file, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity with L1 penalty: 80.29%\nTest score with L1 penalty: 0.8362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example run in 7.441 s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAAFCCAYAAADyjdmjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBAAAskZnCAAAZO3/Aylgab1vIzoAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1db856ef518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Author: Arthur Mensch <arthur.mensch@m4x.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Turn down for faster convergence\n",
    "t0 = time.time()\n",
    "train_samples = 5000\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X = mnist.data.astype('float64')\n",
    "y = mnist.target\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0]) \n",
    "X = X[permutation] #将样本进行随机排列\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1)) #-1表示自动推测维度\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_samples, test_size=10000)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) #使用训练数据的均值方差对训练数据进行标准化\n",
    "X_test = scaler.transform(X_test) #使用训练数据的均值方差对测试数据进行标准化\n",
    "\n",
    "# Turn up tolerance for faster convergence\n",
    "clf = LogisticRegression(C=50. / train_samples,\n",
    "                         multi_class='multinomial',\n",
    "                         penalty='l1', solver='saga', tol=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "sparsity = np.mean(clf.coef_ == 0) * 100  #决定方程中考虑类别时不相关因素的占比，不同类别做平均\n",
    "score = clf.score(X_test, y_test)  #测试集上的平均准确率\n",
    "# print('Best C % .4f' % clf.C_)\n",
    "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "print(\"Test score with L1 penalty: %.4f\" % score)\n",
    "\n",
    "coef = clf.coef_.copy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "scale = np.abs(coef).max()\n",
    "for i in range(10):\n",
    "    l1_plot = plt.subplot(2, 5, i + 1)\n",
    "    l1_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',\n",
    "                   cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n",
    "    l1_plot.set_xticks(())\n",
    "    l1_plot.set_yticks(())\n",
    "    l1_plot.set_xlabel('Class %i' % i)\n",
    "plt.suptitle('Classification vector for...')\n",
    "\n",
    "run_time = time.time() - t0\n",
    "print('Example run in %.3f s' % run_time)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data.astype('float64').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  9.,  9.,  9.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mtrand.RandomState at 0x1db85688d80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_random_state(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84948979591836737"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(clf.coef_[1] ==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Author: Arthur Mensch <arthur.mensch@m4x.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Turn down for faster convergence\n",
    "t0 = time.time()\n",
    "train_samples = 4898*0.6\n",
    "\n",
    "tmp = np.loadtxt(\"winequality-white.csv\", dtype=np.str, delimiter=\";\")\n",
    "X = tmp[1:,0:11].astype(np.float)#加载数据部分\n",
    "y = tmp[1:,11].astype(np.float)#加载类别标签部分\n",
    "\n",
    "random_state = check_random_state(0)       #将样本进行随机排列\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)#使用训练数据的均值方差对训练数据进行标准化\n",
    "X_test = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score with L1 penalty: 0.5112\nExample run in 911.910 s\n"
     ]
    }
   ],
   "source": [
    "# Turn up tolerance for faster convergence\n",
    "clf = LogisticRegression(C=50. / train_samples,\n",
    "                         multi_class='ovr',\n",
    "                         penalty='l1', solver='saga', tol=0.0001)\n",
    "clf.fit(X_train, y_train)\n",
    "#sparsity = np.mean(clf.coef_ == 0) * 100\n",
    "score = clf.score(X_test, y_test)\n",
    "# print('Best C % .4f' % clf.C_)\n",
    "#print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "print(\"Test score with L1 penalty: %.4f\" % score)\n",
    "\n",
    "\n",
    "run_time = time.time() - t0\n",
    "print('Example run in %.3f s' % run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.61\nVariance score: 0.27\nExample run in 644.264 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "regr= linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred =  regr.predict(X_test)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "run_time = time.time() - t0\n",
    "print('Example run in %.3f s' % run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7272d423b0ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msvr_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'poly'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0my_rbf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvr_rbf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0my_lin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvr_lin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0my_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvr_poly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"啊啊啊rbf score: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msvr_rbf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\anaconda\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\anaconda\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    252\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Fit regression model\n",
    "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "svr_lin = SVR(kernel='linear', C=1e3)\n",
    "svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "y_rbf = svr_rbf.fit(X_train, y_train).predict(X_test)\n",
    "y_lin = svr_lin.fit(X_train, y_train).predict(X_test)\n",
    "y_poly = svr_poly.fit(X_train, y_train).predict(X_test)\n",
    "print(\"啊啊啊rbf score: %.4f\" % svr_rbf.score(X_test, y_test))\n",
    "print(\"lin score: %.4f\" % svr_lin.score(X_test, y_test))\n",
    "print(\"poly score: %.4f\" % svr_poly.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"啊啊啊\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
